# Machine Learning Module (Streamlined Best Practice Version)

## 1. Purpose

This Machine Learning (ML) module is a core component of the Carbon Credit Verification SaaS platform. Its primary purpose is to analyze satellite imagery (primarily Sentinel-2 and optionally Sentinel-1 SAR data) to:

*   **Map Forest Cover:** Accurately identify and delineate forested areas.
*   **Detect Forest Change:** Identify and quantify changes in forest cover over time, such as deforestation (loss) or afforestation/reforestation (gain) using advanced techniques like Siamese Networks.
*   **Analyze Temporal Dynamics:** Understand the evolution of forest areas using time-series analysis (e.g., ConvLSTM) to capture complex change patterns.
*   **Estimate Carbon Impact:** Based on detected forest cover and changes, estimate carbon stocks and the CO2 emissions or sequestration resulting from these changes.
*   **Provide Explainable Insights:** Offer transparency into model decisions through Explainable AI (XAI) techniques.

The outputs of this module provide data-driven, auditable evidence to support the verification of carbon credit projects, particularly those related to forestry (e.g., REDD+, ARR).

## 2. ML Pipeline Overview

The ML pipeline within this streamlined package involves several key stages:

1.  **Data Acquisition & Preparation (`ml/utils/data_preparation.py`):
    *   Downloads Sentinel-2 (optical) and Sentinel-1 (SAR, optional) satellite imagery for a user-defined Area of Interest (AOI) and time range using `sentinelsat`.
    *   Downloads Hansen Global Forest Change (GFC) data for training labels.
    *   Performs preprocessing: cloud masking, atmospheric correction (assumed for L2A products), SAR data processing (if included, orchestrated via ESA SNAP), and alignment of all data sources.
    *   Generates analysis-ready data patches (e.g., image tiles, image pairs for change detection, image sequences for time-series analysis).

2.  **Model Training (`ml/training/`):
    *   **Forest Cover Segmentation/Change Detection (U-Net/Siamese UNet):** `train_forest_change.py` (for basic U-Net) and `train_change_detection.py` (for Siamese UNet) train models to segment forest areas or detect changes between image pairs.
    *   **Time-Series Analysis (ConvLSTM):** `train_time_series.py` trains a Convolutional LSTM (ConvLSTM) model to analyze sequences of satellite images, capturing spatio-temporal patterns of forest change.
    *   Trained models are intended to be saved as `.pth` files (typically in a user-created `ml/models/checkpoints/` or similar directory during the actual training execution).

3.  **Inference & Prediction (`ml/inference/`):
    *   `predict_forest_change.py`: Uses trained segmentation/change detection models to predict forest cover or changes on new satellite imagery.
    *   `estimate_carbon_sequestration.py`: Takes forest cover/change maps as input and uses a carbon estimation model (e.g., a Random Forest model that would be trained separately or whose logic is implemented here) along with biomass data or allometric equations to estimate carbon stocks and changes.

4.  **Explainability (XAI) (`ml/utils/xai_visualization.py`):
    *   This script is a placeholder and framework for implementing XAI techniques (e.g., Grad-CAM, SHAP, LIME) to generate explanations for model predictions.
    *   Visualizations of these explanations would be generated by functions within this utility.

## 3. File Structure of this Package (`ml_package_complete.tar.gz`)

```
./
├── ml/                           # Main directory for all ML code
│   ├── data/                   # Placeholder; data preparation logic is in utils/
│   ├── inference/              # Scripts for running predictions
│   │   ├── predict_forest_change.py
│   │   └── estimate_carbon_sequestration.py
│   ├── models/                 # Model architecture definitions
│   │   ├── siamese_unet.py
│   │   └── convlstm_model.py
│   ├── training/               # Scripts for training the ML models
│   │   ├── train_forest_change.py
│   │   ├── train_change_detection.py
│   │   └── train_time_series.py
│   └── utils/                  # Utility scripts
│   │   ├── data_preparation.py # Data download and preprocessing
│   │   ├── visualization.py    # Visualization utilities
│   │   └── xai_visualization.py# XAI utilities
│   └── diagrams/                  # Utility scripts
│       ├── ml_pipeline.dot             # Overall ML pipeline diagram (DOT format)
        ├── ml_pipeline.png             # Overall ML pipeline diagram (PNG image)
        └── revised_ml_section.md     # Detailed documentation of the ML enhancements




```

## 4. Libraries and Technologies

*   **Python 3.8+:** Primary programming language.
*   **PyTorch:** Core deep learning framework for building and training neural networks (Siamese UNet, ConvLSTM).
    *   *Rationale:* Flexible, powerful, widely adopted in research, excellent GPU support.
*   **Rasterio & GDAL:** For reading, writing, and processing geospatial raster data.
    *   *Rationale:* Industry-standard for raster data manipulation.
*   **GeoPandas & Shapely:** For handling vector data (e.g., AOI polygons).
    *   *Rationale:* Extends pandas for spatial operations.
*   **Sentinelsat:** For programmatic download of Sentinel satellite imagery.
    *   *Rationale:* Convenient API for accessing Sentinel data.
*   **ESA SNAP (via `gpt` command-line):** (Optional, if using Sentinel-1) For preprocessing SAR data.
    *   *Rationale:* Official ESA toolbox for Sentinel data processing.
*   **NumPy:** For numerical operations.
    *   *Rationale:* Fundamental for scientific computing.
*   **Scikit-learn:** For traditional ML tasks (e.g., carbon estimation model if Random Forest is used) and evaluation metrics.
    *   *Rationale:* Comprehensive and easy-to-use.
*   **Matplotlib (implied by visualization scripts):** For creating visualizations.
    *   *Rationale:* Widely used for plotting.
*   **Captum (or similar XAI libraries):** Intended for use in `xai_visualization.py`.
    *   *Rationale:* PyTorch-specific library for model interpretability.
*   **Tqdm:** For displaying progress bars.
    *   *Rationale:* Enhances user experience.

## 5. Setup and Execution

For detailed step-by-step instructions on setting up the environment, acquiring data, training models, and evaluating them using this package, please refer to the `cursor_implementation_plan.md` document provided previously. The core scripts to execute will be:

1.  `ml/utils/data_preparation.py` (for data acquisition and preprocessing).
2.  Scripts within `ml/training/` (for model training).
3.  Scripts within `ml/inference/` (for prediction and carbon estimation).

This README provides an overview of the streamlined ML module. For more in-depth information, consult the specific scripts and the `revised_ml_section.md` and `cursor_implementation_plan.md` documents.
