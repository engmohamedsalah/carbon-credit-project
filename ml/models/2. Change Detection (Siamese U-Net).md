# Technical Documentation: Siamese U-Net for Forest Change Detection

This document details the architecture, development process, challenges, and performance of the `change_detection_siamese_unet.pth` model.

## 1. Model Architecture: Siamese U-Net

The model is a **Siamese U-Net**, a deep learning architecture specifically designed for change detection tasks.

-   **U-Net Base:** The core of the model is the U-Net, an encoder-decoder architecture renowned for its effectiveness in image segmentation. It captures multi-scale contextual information through its contracting path (encoder) and precisely localizes changes through its expansive path (decoder) using skip connections.
-   **Siamese Network:** The "Siamese" component consists of two identical U-Net encoders that share the exact same weights. Two input images (e.g., "before" and "after" satellite patches) are passed through these encoders independently. This process generates two feature maps that are directly comparable.
-   **Change Detection Mechanism:** The feature maps from the two encoders are then concatenated and fed into the decoder. The decoder's role is to interpret these combined features and produce a final output mask that highlights the pixels where a significant change has occurred between the two input images.

This architecture was chosen because it is the state-of-the-art for semantic segmentation and its Siamese variant is proven to be highly effective for comparing images and identifying discrepancies.

## 2. Development Journey & Technical Challenges

The project went through several iterations, encountering and overcoming significant technical challenges.

### Challenge 1: Dataset Class Imbalance and Model Bias
-   **Problem:** The initial supervised training used ground truth labels from the Hansen Global Forest Change dataset. Analysis revealed a severe class imbalance: over 98% of the image patches contained no change ("no-change" class). This led to models that were heavily biased. They achieved very high recall by simply predicting "change" everywhere, resulting in near-zero precision and a useless F1-score.
-   **Solution:**
    1.  **Balanced Dataset Creation:** A script (`balance_patch_csv.py`) was created to build a balanced dataset. It selected all available "change" patches and an equal, randomly-sampled number of "no-change" patches. This forced the model to learn from an equal number of positive and negative examples.
    2.  **Focal Loss:** To further combat the effects of imbalance at the pixel level, we switched the loss function to `FocalLoss`. This loss function down-weights the loss assigned to well-classified examples, allowing the model to focus more on the rare "change" pixels, which are harder to learn.

### Challenge 2: Critical Training Loop Bug
-   **Problem:** The first supervised model completely failed to learn (F1 score of 0). A deep dive into the training script (`train_change_detection.py`) revealed that the ground truth label was not being used. The loss was being calculated against a hardcoded `torch.zeros_like(outputs)` tensor, meaning the model was being trained to always predict "no-change".
-   **Solution:** The training loop was corrected to use the actual label mask loaded from the dataset as the target for the loss function. This was a pivotal fix that enabled the model to start learning meaningfully.

### Challenge 3: Loss Function Tuning and Model Performance
-   **Problem:** While Focal Loss was a major improvement, finding the right hyperparameters was key. Initial experiments with `gamma=2` yielded a model with an F1 score of ~0.22. Increasing `gamma` to `3` significantly improved the F1 score to ~0.60, primarily by boosting recall.
-   **Solution:** We experimented with combining `FocalLoss` and `DiceLoss`, which is sensitive to spatial structure. However, both unweighted and weighted combinations failed to outperform the simpler, well-tuned `FocalLoss(gamma=3)`. The final, best-performing model relies solely on Focal Loss.

### Challenge 4: Environment and Codebase Issues
-   **Problem:** Development was hampered by several bugs, including:
    -   Inconsistent model definitions across different files (`ml/training/siamese_unet.py` vs. `ml/models/siamese_unet.py`) causing model loading errors.
    -   Hardcoded file paths in scripts.
    -   A training script that did not correctly parse command-line arguments for saving models, leading to models not being saved.
-   **Solution:** These issues were resolved by deleting the duplicate model file, consolidating the definition, and adding `argparse` to the training script to handle file paths dynamically.

## 3. Final Model and Results

The best performing and final model is `change_detection_siamese_unet.pth`.

-   **Loss Function:** `FocalLoss(alpha=0.5, gamma=3)`
-   **Dataset:** Trained on the balanced dataset (`sentinel2_annual_pairs_balanced.csv`).
-   **Performance:**
    -   **Best F1 Score:** 0.6006 (at threshold 0.4)
    -   **Precision:** 0.4349
    -   **Recall:** 0.9706

The model exhibits very high recall, meaning it is excellent at finding actual change. Its primary weakness is its moderate precision, indicating it still produces a notable number of false positives.

## 4. Future Enhancements

While the current model is a strong baseline, several avenues exist for future improvement:

1.  **Incorporate Sentinel-1 Data:** Fuse Sentinel-2 (optical) data with Sentinel-1 (SAR/radar) data. Radar can penetrate clouds and is sensitive to different ground properties (like moisture and structure), potentially offering complementary information to drastically improve precision and robustness.
2.  **Advanced Augmentation:** Implement more sophisticated data augmentation techniques (e.g., elastic deformations, noise injection) to improve the model's generalization.
3.  **Architectural Improvements:** Explore enhancements to the U-Net architecture, such as incorporating Attention Gates or using more advanced backbones (e.g., ResNet).
4.  **Post-Processing:** Re-evaluate and refine post-processing techniques, like morphological operations, to clean up noisy predictions from the model without sacrificing too much recall.
5.  **Hyperparameter Optimization:** Conduct a more exhaustive search for hyperparameters, particularly for the Adam optimizer (learning rate, weight decay).

## 5. How to Test the Model

To evaluate the performance of the trained model on the test dataset, run the following command from the root directory of the project:

```bash
.venv/bin/python -m ml.inference.evaluate_change_detection --model_path ml/models/change_detection_siamese_unet.pth --data_csv "ml/data/sentinel2_annual_pairs_balanced.csv"
```

This command executes the evaluation script, which loads the specified model and calculates key performance metrics (Precision, Recall, F1-Score) against the provided dataset. The results will be printed to the console.

### Command Breakdown:

-   **`.venv/bin/python`**: This part of the command specifies that we are using the Python interpreter located inside our project's virtual environment (`.venv`). This ensures that we are using the correct versions of all the installed packages (like PyTorch, NumPy, etc.) for this project.

-   **`-m ml.inference.evaluate_change_detection`**: The `-m` flag tells Python to run a module as a script. Here, it's executing the `evaluate_change_detection.py` file, which is located in the `ml/inference/` directory.

-   **`--model_path ml/models/change_detection_siamese_unet.pth`**: This is a command-line argument that tells the evaluation script where to find the trained model weights. The script will load this `.pth` file to reconstruct the state of our best model.

-   **`--data_csv "ml/data/sentinel2_annual_pairs_balanced.csv"`**: This argument points the script to the CSV file that contains the paths to the image pairs and corresponding labels for the test dataset. The script uses this file to load the data it will use to evaluate the model's performance. 